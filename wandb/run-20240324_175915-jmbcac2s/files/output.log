Max Length for sentence = 44
768 0.1 MultiHeadAttentionBlock(
  (w_q): Linear(in_features=768, out_features=768, bias=True)
  (w_k): Linear(in_features=768, out_features=768, bias=True)
  (w_v): Linear(in_features=768, out_features=768, bias=True)
  (w_o): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Traceback (most recent call last):
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 156, in <module>
    train_model(config)
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 93, in train_model
    model = get_model(config, tokenizer.get_vocab_size()).to(device)
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 70, in get_model
    model = build_longformer(
  File "c:\Shaurya\Longformer2\LongformerCoLA\model.py", line 221, in build_longformer
    encoder_block = EncoderBlock_SA(d_model, encoder_self_attention, dropout)
TypeError: __init__() takes 3 positional arguments but 4 were given