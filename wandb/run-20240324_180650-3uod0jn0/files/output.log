Max Length for sentence = 44
768 0.1 MultiHeadAttentionBlock(
  (w_q): Linear(in_features=768, out_features=768, bias=True)
  (w_k): Linear(in_features=768, out_features=768, bias=True)
  (w_v): Linear(in_features=768, out_features=768, bias=True)
  (w_o): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
768 0.1 MultiHeadAttentionBlock(
  (w_q): Linear(in_features=768, out_features=768, bias=True)
  (w_k): Linear(in_features=768, out_features=768, bias=True)
  (w_v): Linear(in_features=768, out_features=768, bias=True)
  (w_o): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
768 0.1 MultiHeadAttentionBlock(
  (w_q): Linear(in_features=768, out_features=768, bias=True)
  (w_k): Linear(in_features=768, out_features=768, bias=True)
  (w_v): Linear(in_features=768, out_features=768, bias=True)
  (w_o): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
Traceback (most recent call last):
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 156, in <module>
    train_model(config)
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 93, in train_model
    model = get_model(config, tokenizer.get_vocab_size()).to(device)
  File "c:\Shaurya\Longformer2\LongformerCoLA\train.py", line 70, in get_model
    model = build_longformer(
  File "c:\Shaurya\Longformer2\LongformerCoLA\model.py", line 235, in build_longformer
    encoder = Encoder(d_model, len(encoder_blocks), nn.ModuleList(encoder_blocks))
  File "c:\Shaurya\Longformer2\LongformerCoLA\model.py", line 185, in __init__
    self.layers = nn.ModuleList([layer_class(features) for _ in range(num_layers)])
TypeError: 'ModuleList' object cannot be interpreted as an integer